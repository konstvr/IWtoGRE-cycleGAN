{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cleared-pillow",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-08-04T08:36:55.991605Z",
     "iopub.status.busy": "2023-08-04T08:36:55.990006Z",
     "iopub.status.idle": "2023-08-04T08:37:01.550389Z",
     "shell.execute_reply": "2023-08-04T08:37:01.549456Z",
     "shell.execute_reply.started": "2023-08-03T14:52:22.942269Z"
    },
    "papermill": {
     "duration": 5.590349,
     "end_time": "2023-08-04T08:37:01.550566",
     "exception": false,
     "start_time": "2023-08-04T08:36:55.960217",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.3.0 and strictly below 2.6.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.10.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras import datasets, layers, optimizers, Sequential, metrics\n",
    "from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2\n",
    "import os\n",
    "import random\n",
    "import imageio\n",
    "\n",
    "from tensorflow import summary\n",
    "from tensorflow.keras.callbacks import TensorBoard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "focal-consideration",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-04T08:37:01.602633Z",
     "iopub.status.busy": "2023-08-04T08:37:01.601732Z",
     "iopub.status.idle": "2023-08-04T08:37:01.604136Z",
     "shell.execute_reply": "2023-08-04T08:37:01.604633Z",
     "shell.execute_reply.started": "2023-08-03T14:52:22.953243Z"
    },
    "papermill": {
     "duration": 0.030716,
     "end_time": "2023-08-04T08:37:01.604808",
     "exception": false,
     "start_time": "2023-08-04T08:37:01.574092",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 9673\n",
    "BATCH_SIZE = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7ee94a",
   "metadata": {},
   "source": [
    "## Load and preprocess PNG images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "innocent-gazette",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-04T08:37:01.661575Z",
     "iopub.status.busy": "2023-08-04T08:37:01.659671Z",
     "iopub.status.idle": "2023-08-04T08:37:01.662265Z",
     "shell.execute_reply": "2023-08-04T08:37:01.662761Z",
     "shell.execute_reply.started": "2023-08-03T14:52:22.972645Z"
    },
    "papermill": {
     "duration": 0.034789,
     "end_time": "2023-08-04T08:37:01.662915",
     "exception": false,
     "start_time": "2023-08-04T08:37:01.628126",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_image(image):\n",
    "    image= tf.image.decode_png(image, channels=1)\n",
    "    image = tf.image.resize(image, [256, 256])\n",
    "    image = (image-127.5)/127.5\n",
    "    return image\n",
    "\n",
    "def load_and_preprocess_image(path):\n",
    "    image = tf.io.read_file(path)\n",
    "    return preprocess_image(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cca2553",
   "metadata": {},
   "source": [
    "## Build proton density image database for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10440f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pd images: 15115\n"
     ]
    }
   ],
   "source": [
    "os.getcwd()\n",
    "\n",
    "all_patient_pd_tse = []\n",
    "\n",
    "#Access patient folders\n",
    "def get_pd_folders(root_dir):\n",
    "    pd_folders = []\n",
    "    for root, _, folders in os.walk(root_dir):\n",
    "        if \"tse2d1_5\" in root:\n",
    "            for pd_img in os.listdir(root):\n",
    "                img_path = os.path.join(root, pd_img)\n",
    "                all_patient_pd_tse.append(img_path)\n",
    "\n",
    "root_dir = \"D:\\\\Vrettos\\\\pngPaired\"\n",
    "pd_image_paths = get_pd_folders(root_dir)\n",
    "\n",
    "print(f\"Total pd images: {len(all_patient_pd_tse)}\")\n",
    "all_patient_pd_tse = [str(path) for path in all_patient_pd_tse[:9673]]\n",
    "ds_T1 = tf.data.Dataset.from_tensor_slices((all_patient_pd_tse))\n",
    "dataset_T1 = ds_T1.map(load_and_preprocess_image)\n",
    "\n",
    "#Test pd_tse database\n",
    "all_test_pd_tse = [str(path) for path in all_patient_pd_tse[-800:]]\n",
    "ds_T1_test = tf.data.Dataset.from_tensor_slices((all_test_pd_tse))\n",
    "dataset_T1_test = ds_T1_test.map(load_and_preprocess_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3118e21",
   "metadata": {},
   "source": [
    "## Build gradient echo image database for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "baf26244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total med images: 15115\n"
     ]
    }
   ],
   "source": [
    "os.getcwd()\n",
    "\n",
    "all_patient_me2d = []\n",
    "\n",
    "#Access patient folders\n",
    "def get_me2d_folders(root_dir):\n",
    "    for root, _, folders in os.walk(root_dir):\n",
    "        if \"me2d1r3\" in root:\n",
    "            for me2d_img in os.listdir(root):\n",
    "                img_path = os.path.join(root, me2d_img)\n",
    "                all_patient_me2d.append(img_path)\n",
    "\n",
    "root_dir = \"D:\\\\Vrettos\\\\pngPaired\"\n",
    "me2d_image_paths = get_me2d_folders(root_dir)\n",
    "\n",
    "print(f\"Total med images: {len(all_patient_me2d)}\")\n",
    "all_patient_me2d = [str(path) for path in all_patient_me2d[:9673]] #80%\n",
    "ds_T2 = tf.data.Dataset.from_tensor_slices((all_patient_me2d))\n",
    "dataset_T2 = ds_T2.map(load_and_preprocess_image)\n",
    "\n",
    "#Test me2d database\n",
    "all_test_me2d = [str(path) for path in all_patient_me2d[-800:]]\n",
    "ds_T2_test = tf.data.Dataset.from_tensor_slices((all_test_me2d))\n",
    "dataset_T2_test = ds_T2_test.map(load_and_preprocess_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95319d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine the two datasets\n",
    "dataset = tf.data.Dataset.zip((dataset_T1, dataset_T2)) .shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "dataset_test = tf.data.Dataset.zip((dataset_T1_test, dataset_T2_test)) .shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "touched-killer",
   "metadata": {
    "papermill": {
     "duration": 0.023061,
     "end_time": "2023-08-04T08:37:35.037873",
     "exception": false,
     "start_time": "2023-08-04T08:37:35.014812",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Create the generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "linear-mississippi",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-04T08:37:35.094961Z",
     "iopub.status.busy": "2023-08-04T08:37:35.094056Z",
     "iopub.status.idle": "2023-08-04T08:37:35.097367Z",
     "shell.execute_reply": "2023-08-04T08:37:35.096886Z",
     "shell.execute_reply.started": "2023-08-03T14:52:26.068357Z"
    },
    "papermill": {
     "duration": 0.035143,
     "end_time": "2023-08-04T08:37:35.097495",
     "exception": false,
     "start_time": "2023-08-04T08:37:35.062352",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "OUTPUT_CHANNELS = 1\n",
    "\n",
    "def downsample(filters, size, apply_instancenorm=True):\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
    "\n",
    "    result = keras.Sequential()\n",
    "    result.add(layers.Conv2D(filters, size, strides=2, padding='same',\n",
    "                             kernel_initializer=initializer, use_bias=False))\n",
    "\n",
    "    if apply_instancenorm:\n",
    "        result.add(tfa.layers.InstanceNormalization(gamma_initializer=gamma_init))\n",
    "\n",
    "    result.add(layers.LeakyReLU())\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dramatic-secret",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-04T08:37:35.151788Z",
     "iopub.status.busy": "2023-08-04T08:37:35.150988Z",
     "iopub.status.idle": "2023-08-04T08:37:35.153607Z",
     "shell.execute_reply": "2023-08-04T08:37:35.154220Z",
     "shell.execute_reply.started": "2023-08-03T14:52:26.088804Z"
    },
    "papermill": {
     "duration": 0.033035,
     "end_time": "2023-08-04T08:37:35.154363",
     "exception": false,
     "start_time": "2023-08-04T08:37:35.121328",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def upsample(filters, size, apply_dropout=False):\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
    "\n",
    "    result = keras.Sequential()\n",
    "    result.add(layers.Conv2DTranspose(filters, size, strides=2,\n",
    "                                      padding='same',\n",
    "                                      kernel_initializer=initializer,\n",
    "                                      use_bias=False))\n",
    "\n",
    "    result.add(tfa.layers.InstanceNormalization(gamma_initializer=gamma_init))\n",
    "\n",
    "    if apply_dropout:\n",
    "        result.add(layers.Dropout(0.5))\n",
    "\n",
    "    result.add(layers.ReLU())\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "starting-burst",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-04T08:37:35.213216Z",
     "iopub.status.busy": "2023-08-04T08:37:35.212378Z",
     "iopub.status.idle": "2023-08-04T08:37:35.215520Z",
     "shell.execute_reply": "2023-08-04T08:37:35.214945Z",
     "shell.execute_reply.started": "2023-08-03T14:52:26.101895Z"
    },
    "papermill": {
     "duration": 0.037872,
     "end_time": "2023-08-04T08:37:35.215653",
     "exception": false,
     "start_time": "2023-08-04T08:37:35.177781",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Generator():\n",
    "    inputs = layers.Input(shape=[256,256,1])\n",
    "\n",
    "    down_stack = [\n",
    "        downsample(64, 4, apply_instancenorm=False),\n",
    "        downsample(128, 4), \n",
    "        downsample(256, 4), \n",
    "        downsample(512, 4), \n",
    "        downsample(512, 4), \n",
    "        downsample(512, 4), \n",
    "        downsample(512, 4),\n",
    "        downsample(512, 4),\n",
    "    ]\n",
    "\n",
    "    up_stack = [\n",
    "        upsample(512, 4, apply_dropout=True), \n",
    "        upsample(512, 4, apply_dropout=True), \n",
    "        upsample(512, 4, apply_dropout=True), \n",
    "        upsample(512, 4), \n",
    "        upsample(256, 4), \n",
    "        upsample(128, 4), \n",
    "        upsample(64, 4),\n",
    "    ]\n",
    "\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    last = layers.Conv2DTranspose(OUTPUT_CHANNELS, 4,\n",
    "                                  strides=2,\n",
    "                                  padding='same',\n",
    "                                  kernel_initializer=initializer,\n",
    "                                  activation='tanh') \n",
    "\n",
    "    x = inputs\n",
    "\n",
    "    # Downsample\n",
    "    skips = []\n",
    "    for down in down_stack:\n",
    "        x = down(x)\n",
    "        skips.append(x)\n",
    "\n",
    "    skips = reversed(skips[:-1])\n",
    "\n",
    "    # Upsample and skip connections\n",
    "    for up, skip in zip(up_stack, skips):\n",
    "        x = up(x)\n",
    "        x = layers.Concatenate()([x, skip])\n",
    "\n",
    "    x = last(x)\n",
    "\n",
    "    return keras.Model(inputs=inputs, outputs=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wanted-champion",
   "metadata": {
    "papermill": {
     "duration": 0.026273,
     "end_time": "2023-08-04T08:37:35.264893",
     "exception": false,
     "start_time": "2023-08-04T08:37:35.238620",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Create the discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "northern-swiss",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-04T08:37:35.324032Z",
     "iopub.status.busy": "2023-08-04T08:37:35.322062Z",
     "iopub.status.idle": "2023-08-04T08:37:35.324713Z",
     "shell.execute_reply": "2023-08-04T08:37:35.325237Z",
     "shell.execute_reply.started": "2023-08-03T14:52:26.119855Z"
    },
    "papermill": {
     "duration": 0.036937,
     "end_time": "2023-08-04T08:37:35.325402",
     "exception": false,
     "start_time": "2023-08-04T08:37:35.288465",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Discriminator():\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
    "\n",
    "    inp = layers.Input(shape=[256, 256, 1], name='input_image')\n",
    "\n",
    "    x = inp\n",
    "\n",
    "    down1 = downsample(64, 4, False)(x) \n",
    "    down2 = downsample(128, 4)(down1) \n",
    "    down3 = downsample(256, 4)(down2) \n",
    "\n",
    "    zero_pad1 = layers.ZeroPadding2D()(down3) \n",
    "    conv = layers.Conv2D(512, 4, strides=1,\n",
    "                         kernel_initializer=initializer,\n",
    "                         use_bias=False)(zero_pad1) \n",
    "\n",
    "    norm1 = tfa.layers.InstanceNormalization(gamma_initializer=gamma_init)(conv)\n",
    "\n",
    "    leaky_relu = layers.LeakyReLU()(norm1)\n",
    "\n",
    "    zero_pad2 = layers.ZeroPadding2D()(leaky_relu)\n",
    "\n",
    "    last = layers.Conv2D(1, 4, strides=1,\n",
    "                         kernel_initializer=initializer)(zero_pad2)\n",
    "\n",
    "    return tf.keras.Model(inputs=inp, outputs=last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "preceding-watershed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-04T08:37:35.377500Z",
     "iopub.status.busy": "2023-08-04T08:37:35.376747Z",
     "iopub.status.idle": "2023-08-04T08:37:38.545154Z",
     "shell.execute_reply": "2023-08-04T08:37:38.544537Z",
     "shell.execute_reply.started": "2023-08-03T14:52:26.140296Z"
    },
    "papermill": {
     "duration": 3.196096,
     "end_time": "2023-08-04T08:37:38.545313",
     "exception": false,
     "start_time": "2023-08-04T08:37:35.349217",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "pd_generator = Generator() # transforms T2 to T1\n",
    "med_generator = Generator() # transforms T1 paintings to be T2\n",
    "\n",
    "pd_discriminator = Discriminator() # differentiates real T1 and generated T1\n",
    "med_discriminator = Discriminator() # differentiates real T2 and generated T2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "italic-addition",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-04T08:37:38.696463Z",
     "iopub.status.busy": "2023-08-04T08:37:38.695543Z",
     "iopub.status.idle": "2023-08-04T08:37:38.716427Z",
     "shell.execute_reply": "2023-08-04T08:37:38.715925Z",
     "shell.execute_reply.started": "2023-08-03T14:52:30.665879Z"
    },
    "papermill": {
     "duration": 0.05319,
     "end_time": "2023-08-04T08:37:38.716548",
     "exception": false,
     "start_time": "2023-08-04T08:37:38.663358",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CycleGan(keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        pd_generator,\n",
    "        med_generator,\n",
    "        pd_discriminator,\n",
    "        med_discriminator,\n",
    "        lambda_cycle=10,\n",
    "    ):\n",
    "        super(CycleGan, self).__init__()\n",
    "        self.pd_gen = pd_generator\n",
    "        self.med_gen = med_generator\n",
    "        self.pd_disc = pd_discriminator\n",
    "        self.med_disc = med_discriminator\n",
    "        self.lambda_cycle = lambda_cycle\n",
    "        \n",
    "    def compile(\n",
    "        self,\n",
    "        pd_gen_optimizer,\n",
    "        med_gen_optimizer,\n",
    "        pd_disc_optimizer,\n",
    "        med_disc_optimizer,\n",
    "        gen_loss_fn,\n",
    "        disc_loss_fn,\n",
    "        cycle_loss_fn,\n",
    "        identity_loss_fn\n",
    "    ):\n",
    "        super(CycleGan, self).compile()\n",
    "        self.pd_gen_optimizer = pd_gen_optimizer\n",
    "        self.med_gen_optimizer = med_gen_optimizer\n",
    "        self.pd_disc_optimizer = pd_disc_optimizer\n",
    "        self.med_disc_optimizer = med_disc_optimizer\n",
    "        self.gen_loss_fn = gen_loss_fn\n",
    "        self.disc_loss_fn = disc_loss_fn\n",
    "        self.cycle_loss_fn = cycle_loss_fn\n",
    "        self.identity_loss_fn = identity_loss_fn\n",
    "        \n",
    "    def train_step(self, batch_data):\n",
    "        real_pd, real_med = batch_data\n",
    "        \n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            # photo to monet back to photo\n",
    "            fake_pd = self.pd_gen(real_med, training=True)\n",
    "            cycled_med = self.med_gen(fake_pd, training=True)\n",
    "\n",
    "            # monet to photo back to monet\n",
    "            fake_med = self.med_gen(real_pd, training=True)\n",
    "            cycled_pd = self.pd_gen(fake_med, training=True)\n",
    "\n",
    "            # generating itself\n",
    "            same_pd = self.pd_gen(real_pd, training=True)\n",
    "            same_med = self.med_gen(real_med, training=True)\n",
    "\n",
    "            # discriminator used to check, inputing real images\n",
    "            disc_real_pd = self.pd_disc(real_pd, training=True)\n",
    "            disc_real_med = self.med_disc(real_med, training=True)\n",
    "\n",
    "            # discriminator used to check, inputing fake images\n",
    "            disc_fake_pd = self.pd_disc(fake_pd, training=True)\n",
    "            disc_fake_med = self.med_disc(fake_med, training=True)\n",
    "\n",
    "            # evaluates generator loss\n",
    "            pd_gen_loss = self.gen_loss_fn(disc_fake_pd)\n",
    "            med_gen_loss = self.gen_loss_fn(disc_fake_med)\n",
    "\n",
    "            # evaluates total cycle consistency loss\n",
    "            total_cycle_loss = self.cycle_loss_fn(real_pd, cycled_pd, self.lambda_cycle) + self.cycle_loss_fn(real_med, cycled_med, self.lambda_cycle)\n",
    "\n",
    "            # evaluates total generator loss\n",
    "            total_pd_gen_loss = pd_gen_loss + total_cycle_loss + self.identity_loss_fn(real_pd, same_pd, self.lambda_cycle)\n",
    "            total_med_gen_loss = med_gen_loss + total_cycle_loss + self.identity_loss_fn(real_med, same_med, self.lambda_cycle)\n",
    "\n",
    "            # evaluates discriminator loss\n",
    "            pd_disc_loss = self.disc_loss_fn(disc_real_pd, disc_fake_pd)\n",
    "            med_disc_loss = self.disc_loss_fn(disc_real_med, disc_fake_med)\n",
    "\n",
    "        # Calculate the gradients for generator and discriminator\n",
    "        pd_generator_gradients = tape.gradient(total_pd_gen_loss,\n",
    "                                                  self.pd_gen.trainable_variables)\n",
    "        med_generator_gradients = tape.gradient(total_med_gen_loss,\n",
    "                                                  self.med_gen.trainable_variables)\n",
    "\n",
    "        pd_discriminator_gradients = tape.gradient(pd_disc_loss,\n",
    "                                                      self.pd_disc.trainable_variables)\n",
    "        med_discriminator_gradients = tape.gradient(med_disc_loss,\n",
    "                                                      self.med_disc.trainable_variables)\n",
    "\n",
    "        # Apply the gradients to the optimizer\n",
    "        self.pd_gen_optimizer.apply_gradients(zip(pd_generator_gradients,\n",
    "                                                 self.pd_gen.trainable_variables))\n",
    "\n",
    "        self.med_gen_optimizer.apply_gradients(zip(med_generator_gradients,\n",
    "                                                 self.med_gen.trainable_variables))\n",
    "\n",
    "        self.pd_disc_optimizer.apply_gradients(zip(pd_discriminator_gradients,\n",
    "                                                  self.pd_disc.trainable_variables))\n",
    "\n",
    "        self.med_disc_optimizer.apply_gradients(zip(med_discriminator_gradients,\n",
    "                                                  self.med_disc.trainable_variables))\n",
    "        \n",
    "        return {\n",
    "            \"pd_gen_loss\": total_pd_gen_loss,\n",
    "            \"med_gen_loss\": total_med_gen_loss,\n",
    "            \"pd_disc_loss\": pd_disc_loss,\n",
    "            \"med_disc_loss\": med_disc_loss\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "retained-wheat",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-04T08:37:38.768712Z",
     "iopub.status.busy": "2023-08-04T08:37:38.767936Z",
     "iopub.status.idle": "2023-08-04T08:37:38.770551Z",
     "shell.execute_reply": "2023-08-04T08:37:38.771016Z",
     "shell.execute_reply.started": "2023-08-03T14:52:30.693063Z"
    },
    "papermill": {
     "duration": 0.031556,
     "end_time": "2023-08-04T08:37:38.771163",
     "exception": false,
     "start_time": "2023-08-04T08:37:38.739607",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def discriminator_loss(real, generated):\n",
    "     real_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(tf.ones_like(real), real)\n",
    "\n",
    "     generated_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(tf.zeros_like(generated), generated)\n",
    "\n",
    "     total_disc_loss = real_loss + generated_loss\n",
    "\n",
    "     return tf.reduce_mean(total_disc_loss * 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "peaceful-panic",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-04T08:37:38.822505Z",
     "iopub.status.busy": "2023-08-04T08:37:38.821912Z",
     "iopub.status.idle": "2023-08-04T08:37:38.826040Z",
     "shell.execute_reply": "2023-08-04T08:37:38.825312Z",
     "shell.execute_reply.started": "2023-08-03T14:52:30.709479Z"
    },
    "papermill": {
     "duration": 0.031795,
     "end_time": "2023-08-04T08:37:38.826181",
     "exception": false,
     "start_time": "2023-08-04T08:37:38.794386",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generator_loss(generated):\n",
    "    return tf.reduce_mean(tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(tf.ones_like(generated), generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "occupied-relief",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-04T08:37:38.879301Z",
     "iopub.status.busy": "2023-08-04T08:37:38.878338Z",
     "iopub.status.idle": "2023-08-04T08:37:38.881194Z",
     "shell.execute_reply": "2023-08-04T08:37:38.880717Z",
     "shell.execute_reply.started": "2023-08-03T14:52:30.728669Z"
    },
    "papermill": {
     "duration": 0.031405,
     "end_time": "2023-08-04T08:37:38.881317",
     "exception": false,
     "start_time": "2023-08-04T08:37:38.849912",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calc_cycle_loss(real_image, cycled_image, LAMBDA):\n",
    "    loss1 = tf.reduce_mean(tf.abs(real_image - cycled_image))\n",
    "\n",
    "    return LAMBDA * loss1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "vanilla-brain",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-04T08:37:38.933454Z",
     "iopub.status.busy": "2023-08-04T08:37:38.932867Z",
     "iopub.status.idle": "2023-08-04T08:37:38.936871Z",
     "shell.execute_reply": "2023-08-04T08:37:38.936383Z",
     "shell.execute_reply.started": "2023-08-03T14:52:30.743902Z"
    },
    "papermill": {
     "duration": 0.03168,
     "end_time": "2023-08-04T08:37:38.936994",
     "exception": false,
     "start_time": "2023-08-04T08:37:38.905314",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def identity_loss(real_image, same_image, LAMBDA):\n",
    "    loss = tf.reduce_mean(tf.abs(real_image - same_image))\n",
    "    return LAMBDA * 0.5 * loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "unauthorized-folks",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-04T08:37:38.989220Z",
     "iopub.status.busy": "2023-08-04T08:37:38.988298Z",
     "iopub.status.idle": "2023-08-04T08:37:38.991852Z",
     "shell.execute_reply": "2023-08-04T08:37:38.991315Z",
     "shell.execute_reply.started": "2023-08-03T14:52:30.758052Z"
    },
    "papermill": {
     "duration": 0.032169,
     "end_time": "2023-08-04T08:37:38.991978",
     "exception": false,
     "start_time": "2023-08-04T08:37:38.959809",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd_generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "med_generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "\n",
    "pd_discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "med_discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "sophisticated-generation",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-04T08:37:39.045108Z",
     "iopub.status.busy": "2023-08-04T08:37:39.044163Z",
     "iopub.status.idle": "2023-08-04T08:37:39.061973Z",
     "shell.execute_reply": "2023-08-04T08:37:39.062581Z",
     "shell.execute_reply.started": "2023-08-03T14:52:30.773738Z"
    },
    "papermill": {
     "duration": 0.0479,
     "end_time": "2023-08-04T08:37:39.062745",
     "exception": false,
     "start_time": "2023-08-04T08:37:39.014845",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cycle_gan_model = CycleGan(\n",
    "    pd_generator, med_generator, pd_discriminator, med_discriminator\n",
    "    )\n",
    "\n",
    "cycle_gan_model.compile(\n",
    "    pd_gen_optimizer = pd_generator_optimizer,\n",
    "    med_gen_optimizer = med_generator_optimizer,\n",
    "    pd_disc_optimizer = pd_discriminator_optimizer,\n",
    "    med_disc_optimizer = med_discriminator_optimizer,\n",
    "    gen_loss_fn = generator_loss,\n",
    "    disc_loss_fn = discriminator_loss,\n",
    "    cycle_loss_fn = calc_cycle_loss,\n",
    "    identity_loss_fn = identity_loss,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b67c32d",
   "metadata": {},
   "source": [
    "## Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "neutral-chest",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-04T08:37:39.118031Z",
     "iopub.status.busy": "2023-08-04T08:37:39.117283Z",
     "iopub.status.idle": "2023-08-04T12:23:03.266956Z",
     "shell.execute_reply": "2023-08-04T12:23:03.266422Z",
     "shell.execute_reply.started": "2023-08-03T14:52:30.803429Z"
    },
    "papermill": {
     "duration": 13524.18015,
     "end_time": "2023-08-04T12:23:03.267115",
     "exception": false,
     "start_time": "2023-08-04T08:37:39.086965",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 9\u001b[0m\n\u001b[0;32m      1\u001b[0m tensorboard_callback \u001b[38;5;241m=\u001b[39m TensorBoard(log_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodelLogs\u001b[39m\u001b[38;5;124m\"\u001b[39m, histogram_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      2\u001b[0m model_checkpoint_callback \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mModelCheckpoint(\n\u001b[0;32m      3\u001b[0m     filepath\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodelCheckpoints/weights.\u001b[39m\u001b[38;5;132;01m{epoch:02d}\u001b[39;00m\u001b[38;5;124m.hdf5\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      4\u001b[0m     save_weights_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m      5\u001b[0m     monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmed_gen_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      6\u001b[0m     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      7\u001b[0m     save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 9\u001b[0m \u001b[43mcycle_gan_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m150\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mtensorboard_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel_checkpoint_callback\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tensorboard_callback = TensorBoard(log_dir=\"modelLogs\", histogram_freq=1)\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=\"modelCheckpoints/weights.{epoch:02d}.hdf5\",\n",
    "    save_weights_only=True,\n",
    "    monitor=\"med_gen_loss\",\n",
    "    mode='min',\n",
    "    save_best_only=True)\n",
    "\n",
    "cycle_gan_model.fit(\n",
    "    dataset,\n",
    "    epochs=150,\n",
    "    callbacks = [tensorboard_callback,model_checkpoint_callback]\n",
    ")       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880dc35c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 13708.366741,
   "end_time": "2023-08-04T12:25:18.799462",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-08-04T08:36:50.432721",
   "version": "2.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
